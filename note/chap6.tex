\section{Duality} % (fold)
\label{sec:duality}

\subsection{Weak Duality} % (fold)
\label{sub:weak_duality}

\begin{remark}
	Gradient Descent works for unconstrained optimization problem on convex functions. It has challenges that the function should be differentiable and convex, and at the same time GD is computationally expensive. 

	Duality is a technique that transforms every problem to a convex form (the dual form). It does not necessarily give us the solution to the original problem, but it will always give a bound.
\end{remark}

\begin{theorem}[Lagrangian]
	Optimization problem
	\[
p^* = \min f_0(\v{x})
	\]
	Under the constrains
	\begin{align*}
		f_i(\v{x})&\le0,\;\;\;\forall i\;\;1\le i\le m\\
		h_i(\v{x})&=0,\;\;\;\forall i\;\;1\le i\le p
	\end{align*}
	The problem defined above is called a \textbf{primal problem}. Define \textbf{Lagrangian}:
	\[
L(\v{x},\v{\lambda},\v{\nu}) = f_0(\v{x})+\sum_{i=1}^m \lambda_if_i(\v{x})+\sum_{i=1}^p\nu_ih_i(\v{x}) \;\;\; \forall \lambda_i \ge 0
	\]
	Note that it is an \textbf{affine} function of $\lambda$ and $\nu$, which means that it is \textbf{both convex and concave}. Now, define new problem
	\[
\min_{\lambda \ge 0} L(\v{x},\v{\lambda},\v{\nu}) := g(\v{\lambda},\v{\nu})
	\]
	Over all $\v{x}$. Examine g. Properties of g include
	\begin{enumerate}
		\item g is a function of only $\v{\lambda},\v{\nu}$.
		\item L is an affine function of $\v{\lambda},\v{\nu}$.
		\item By 1 and 2, g is a concave function of $\v{\lambda},\v{\nu}$.
		\item It turns out, \textbf{g($\v{\lambda},\v{\nu}$) is a lower bound on the primal optimal $p^*$}.
	\end{enumerate}
\end{theorem}
Proof of property No. 4: TODO

\begin{example}[Duality on LS]
	Let $A\in\mathbb{R}^{m*n}$, $m<n$. Problem
	\[
\min \v{x}^\top\v{x} = \v{p}^*
	\]
	Under the constrain
	\[
A\v{x} = \v{b}
	\]
	Since there is no inequality constains, the lagrangian of this problem does not have $\lambda$.
	\[
L(\v{x},\v{\nu}) = \v{x}^\top\v{x} + \nu^\top(A\v{x}-\v{b})
	\]
	\[
g(\v{\nu}) = \min_{\v{x}}L(\v{x},\v{\nu})
	\]
	To solve for g, we do
	\[
\nabla_{\v{x}} L(\v{x},\v{\nu}) = 2\v{x}+A^\top\v{\nu}
	\]
	Set gradient to zero,
	\[
\v{x} = -\frac{1}{2}A^\top\v{\nu}
	\]
	Is the point where L is minimized. g at this point is 
	\begin{align*}
		g(\v{\nu}) &= L(-\frac{1}{2}A^\top\v{\nu}, \v{\nu}) = (-\frac{1}{2}A^\top\v{\nu})^\top(-\frac{1}{2}A^\top\v{\nu})+\v{\nu}^\top(-\frac{1}{2}AA^\top\v{\nu}-\v{b})\\
		&=\frac{1}{4}\v{\nu}^\top A A^\top\v{\nu} +\v{\nu}^\top(-AA^\top\v{\nu}-\v{b})\\
		&= -\frac{1}{4}\v{\nu}^\top A A^\top\v{\nu} - \v{\nu}^\top\v{b}
	\end{align*}
	Which is the lower bound of $p^*$. What is the max lower bound? In another word, we want to find
	\[
\max_{\v{\nu}}g(\v{\nu})
	\]
	Since g concave, we take its gradient and set to zero:
	\begin{align*}
		\nabla_{\v{\nu}} g(\v{\nu}) &= -\frac{1}{4}2AA^\top\v{\nu}-\v{b} = 0\\
		\v{\nu}^* &= -2(AA^\top)^{-1}\v{b}
	\end{align*}
	Thus
	\[
\v{x}^* = -\frac{1}{2}A^\top(-2(AA^\top)^{-1}\v{b}) = A^\top(AA^\top)^{-1}\v{b}
	\]
\end{example}

\begin{definition}[Dual]
	Continuing the definition of lagrangian, we want to find the tightest lower bound of $p^*$, formally
	\[
\max_{\v{\lambda}\ge0}g(\v{\lambda},\v{\nu}) = d^*
	\]
	Over $\v{\nu}$, this problem is the \textbf{DUAL Problem}. It is a maximization problem of a concave function under linear constrains, thus a \textbf{convex program}.
\end{definition}

\begin{remark}
	Properties of the Dual problem:
	\begin{enumerate}
		\item \# of variables = \# of constraints of the primal.
		\item \textbf{Always convex problem even if primal is not! :)}
	\end{enumerate}
	By 1, if the data is large but the constaints are few, the Dual problem is an easier problem to solve.
\end{remark}

\begin{definition}[Weak Duality]
	Continuing from the definition of lagrangian and duality, if
	\[
d^*\le p^*
	\]
	Then it is \textbf{WEAK DUALITY}.
\end{definition}

% subsection weak_duality (end)

\subsection{Strong Duality} % (fold)
\label{sub:strong_duality}

\begin{definition}[Strong Duality]
	If
	\[
d^*= p^*
	\]
	Like we seen in the Duality on LS example, then it is \textbf{STRONG DUALITY}.
\end{definition}

\begin{remark}
	Strong Duality $\implies$ Weak Duality. Weak duality always holds, while strong duality only holds under certain circumstances.
\end{remark}

% subsection strong_duality (end)

% section duality (end)